{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noble-force",
   "metadata": {},
   "source": [
    "# A notebook to explore text classification using word embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-eclipse",
   "metadata": {},
   "source": [
    "In this notebook, I will explore taking a public dataset of books with metadata such as description, title and category/genre. \n",
    "Ill then use a word embedder to vectorize the description and title and then use XGBoost to create a classifier on the category. \n",
    "I will use GenSim's fasttext implementation as the word embedder to vectorize the description and title. \n",
    "I will then repeat this process but using the native FastText implementation and compare the results. \n",
    "I will then host these models on Amazon's SageMaker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-excess",
   "metadata": {},
   "source": [
    "todo:\n",
    "Clean up, remove redundant stuff, remove the weights, update any ilocs whcih expected the weight to be in there. \n",
    "can remove the model training locally as all done in the cloud. \n",
    "Did we import stuff more than once\n",
    "Did we use different ways to call sessions for instance\n",
    "Might need to check in sublime for repeats \n",
    "Lets add the confusion matrix on this. \n",
    "\n",
    "\n",
    "Maybe lets create a table of contents where we have:\n",
    "Feature engineering - description of the dataset\n",
    "Training a model with gensim, and xgboost to do the classification.\n",
    "Training a model, push model to S3, and host it on a XGBoost container - doesn't work\n",
    "Training a model with FastText\n",
    "Hosting this model in SageMaker using Blazing Text\n",
    "\n",
    "TOC - There is an ipython nbextension that constructs a table of contents for a notebook. It seems to only provide navigation, not section folding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-district",
   "metadata": {},
   "source": [
    "## Install libraries, initialise variables, download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "from gensim.utils import simple_preprocess\n",
    "print(common_texts[1])\n",
    "print(len(common_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-queensland",
   "metadata": {},
   "source": [
    "gemsim expects the sentences to already be tokenized and pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gensim.models.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() # replace with your own bucket if you have one \n",
    "s3 = sagemaker_session.boto_session.resource('s3')\n",
    "\n",
    "\n",
    "prefix_gensim = 'data_gensim_xgb'\n",
    "prefix_fasttext = 'data_fasttext'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-render",
   "metadata": {},
   "source": [
    "## Get the data into a working format with just the features we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the book metadata\n",
    "! wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles/meta_Books.json.gz\n",
    "# Uncompressing\n",
    "!gzip -d meta_Books.json.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-photographer",
   "metadata": {},
   "source": [
    "The filesize is a bit too big, so we can reduce that if the below line by taking a subset of that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the dataset \n",
    "! head -n 100000 meta_Books.json > books_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data=pd.read_json('books_train.json', lines=True)\n",
    "#shuffle the data in place\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# show first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-missouri",
   "metadata": {},
   "source": [
    "We are only interested in a few columns from this dataset, so we will create a dataframe that onyl returns these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data[[\"category\",\"description\", \"title\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-biotechnology",
   "metadata": {},
   "source": [
    "We will do some analysis of the data we have here to see how the data looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data_subset.category.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "length.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cnt_cats\"] = data_subset.category.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cnt_desc\"] = data_subset.description.apply(len)\n",
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the rows that have no category\n",
    "data_subset = data_subset[data_subset.cnt_cats != 0]\n",
    "data_subset = data_subset[data_subset.cnt_desc != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"category\"].str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-article",
   "metadata": {},
   "source": [
    "We can see that the category column has an array which is a hierachy classification of the book. We can train our classifer on just one of those, they are all books, so no need to be interested in the first element, but the second element looks more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-portfolio",
   "metadata": {},
   "source": [
    "We just want to clean some of the data as we can see there was some encoding issues whcih we can fix with a \"replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"cat_x2\"].replace(\"&amp;\", \"&\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_subset[\"cat_x2\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['description_str'] = data_subset['description'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-apparatus",
   "metadata": {},
   "source": [
    "We want to update the category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"] = data_subset[\"cat_x2\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"cat_x2_code\"] = data_subset[\"cat_x2\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-tuition",
   "metadata": {},
   "source": [
    "## GenSim requires us to do some cleansing of the data and tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text): \n",
    "    '''  \n",
    "    This function takes strings containing numbers and returns strings with numbers removed.\n",
    "    '''\n",
    "    return re.sub(r'\\d+', '', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    '''  \n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    mentions (@ and the account name) removed.\n",
    "    Input(string): one tweet, contains mentions\n",
    "    Output(string): one tweet, mentions (@ and the account name mentioned) removed \n",
    "    '''\n",
    "    mentions = re.compile(r'@\\w+ ?')\n",
    "    return mentions.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(text):\n",
    "    '''\n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    mentions (@ and the account name) extracted into a different element,\n",
    "    and removes the mentions in the original sentence.\n",
    "    Input(string): one sentence, contains mentions\n",
    "    Output:\n",
    "    one tweet (string): mentions (@ and the account name mentioned) removed \n",
    "    mentions (string): (only the account name mentioned) extracted\n",
    "    '''\n",
    "    mentions = [i[1:] for i in text.split() if i.startswith(\"@\")]\n",
    "    sentence = re.compile(r'@\\w+ ?').sub(r'', text)\n",
    "    return sentence,mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_list = string.punctuation #you can self define list of punctuation to remove here\n",
    "def remove_punctuation(text): \n",
    "    \"\"\"\n",
    "    This function takes strings containing self defined punctuations and returns\n",
    "    strings with punctuations removed.\n",
    "    Input(string): one tweet, contains punctuations in the self-defined list\n",
    "    Output(string): one tweet, self-defined punctuations removed \n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', punc_list) \n",
    "    return text.translate(translator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text): \n",
    "    '''\n",
    "    This function takes strings containing mentions and returns strings with \n",
    "    whitespaces removed.\n",
    "    Input(string): one tweet, contains whitespaces\n",
    "    Output(string): one tweet, white spaces removed \n",
    "    '''\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str\"] = data_subset[\"description_str\"].str.lower()\n",
    "data_subset[\"title\"] = data_subset[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str\"]=data_subset[\"description_str\"].apply(remove_whitespace).apply(remove_punctuation).apply(remove_numbers)\n",
    "data_subset[\"title\"]=data_subset[\"title\"].apply(remove_whitespace).apply(remove_punctuation).apply(remove_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "def tokenize_sent(text): \n",
    "    ''' \n",
    "    This function takes strings (a tweet) and returns tokenized words.\n",
    "    Input(string): one tweet\n",
    "    Output(list): list of words tokenized from the tweet\n",
    "    '''\n",
    "    word_tokens = word_tokenize(text)  \n",
    "    return word_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str_token\"] = data_subset[\"description_str\"].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"title_token\"] = data_subset[\"title\"].apply(tokenize_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for word in  [w for sent in data_subset[\"description_str_token\"] for w in sent]:\n",
    "    counter[word] += 1        \n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#least frequent words\n",
    "counter.most_common()[:-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "bottom_n = 10\n",
    "stopwords_list |= set([word for (word, count) in counter.most_common(top_n)])\n",
    "stopwords_list |= set([word for (word, count) in counter.most_common()[:-bottom_n:-1]])\n",
    "stopwords_list |= {'thats'}\n",
    "def remove_stopwords(tokenized_text): \n",
    "    '''\n",
    "    This function takes a list of tokenized words from the description and title, removes self-defined stop words from the list,\n",
    "    and returns the list of words with stop words removed\n",
    "    '''\n",
    "    filtered_text = [word for word in tokenized_text if word not in stopwords_list] \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want to make sure we dont remove words we want to keep so check the results above\n",
    "data_subset[\"description_str_token\"] = data_subset[\"description_str_token\"].apply(remove_stopwords)\n",
    "data_subset[\"title_token\"] = data_subset[\"title_token\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller(lang='en', fast = True)\n",
    "def spelling_correct(tokenized_text):\n",
    "    \"\"\"\n",
    "    This function takes a list of tokenized words from a tweet, spell check every words and returns the \n",
    "    corrected words if applicable. Note that not every wrong spelling words will be identified especially \n",
    "    for tweets.\n",
    "    Input(list): a list of tokenized words from a tweet, contains wrong-spelling words\n",
    "    Output(list): a list of corrected words \n",
    "    \"\"\"\n",
    "    corrected = [spell(word) for word in tokenized_text] \n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset[\"description_str_token\"] = data_subset[\"description_str_token\"].apply(spelling_correct)\n",
    "data_subset[\"title_token\"] = data_subset[\"title_token\"].apply(spelling_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset['description_str'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows which don't have data\n",
    "data_subset = data_subset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-bargain",
   "metadata": {},
   "source": [
    "### Now data has been cleansed, wee are ready to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-twins",
   "metadata": {},
   "source": [
    "We will see when we return a sentence in it's vectorized format, we will have an array of 50 items, as that is the size we have choosen, where this is capturing the semantics of the sentence, and that will enable us to compare 2 sentences and see how similar they are for instance, and for this use-case, to be able to train a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim = FastText(size=50, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_desc = data_subset[\"description_str_token\"] + data_subset[\"title_token\"]\n",
    "token_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim.build_vocab(sentences=token_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gensim.train(sentences=token_desc, total_examples=len(token_desc), epochs=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"fasttext.model\")\n",
    "\n",
    "model_gensim.save('books_gensim_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_str = data_subset[\"description_str\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str = model_gensim.wv[description_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_description_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str = np.split(vector_description_str,len(vector_description_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_str = data_subset[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str = model_gensim.wv[title_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_title_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str = np.split(vector_title_str,len(vector_title_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_desc_title = np.concatenate((vector_title_str, vector_description_str), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_title_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_description_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_desc_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_desc_title.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-sherman",
   "metadata": {},
   "source": [
    "We want to reshape the vector into a 2D with same number of rows and concatenating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_vector_title_descr = vector_desc_title.reshape(len(vector_title_str),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_vector_title_descr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_vector_title_descr = pd.DataFrame(data=big_vector_title_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big_vector_title_descr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-colon",
   "metadata": {},
   "source": [
    "Our index on both these DataFrames wont align anymore, so we need to reset the index so we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data_subset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2 = pd.concat([data_subset, df_big_vector_title_descr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-producer",
   "metadata": {},
   "source": [
    "### We want to check the count of each of the classes to check for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-partition",
   "metadata": {},
   "source": [
    "With another version of XGBoost, we can supply the weights as a vector as a parameter for the training which will improve the model training to help the model be less bias because of the class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2['cat_x2_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2_cat_x2_agg = data_subset_2.groupby(by=['cat_x2_code']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-chemistry",
   "metadata": {},
   "source": [
    "Get the data in the format ready for fasttext too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2[\"fastText_label\"] = '__label__' + data_subset[\"cat_x2_code\"].astype(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-melissa",
   "metadata": {},
   "source": [
    "We have our data in a format that we like now, but for the training, we can select a few columns for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-sequence",
   "metadata": {},
   "source": [
    "Might be better to pick the columns, rather than drop so many, lets look at the head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe before saving the data as CSV\n",
    "df_gensim_xgb_sampleweight = data_subset_2.drop(columns=['index','category','description','title','cnt_cats','cnt_desc','cat_x2','description_str','description_str_token','title_token','fastText_label'])\n",
    "df_fasttext = data_subset_2[['fastText_label','description_str', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gensim_xgb_sampleweight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-difference",
   "metadata": {},
   "source": [
    "### For this version of XGBoost, we need to supply 3 arguments to the model which is the features, labels and optionally the sample weight which is going to help improve the performance of the model as we have an imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-projection",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-amsterdam",
   "metadata": {},
   "source": [
    "# Testing bringing our own script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_gensim_xgb_sampleweight.drop(['cat_x2_code'], axis=1).values\n",
    "y = df_gensim_xgb_sampleweight['cat_x2_code'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the dataframes to csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "yX_train = np.column_stack((y_train, X_train))\n",
    "yX_test = np.column_stack((y_test, X_test))\n",
    "np.savetxt(\"book_gensim_train_v1.csv\", yX_train, delimiter=\",\", fmt='%0.3f')\n",
    "np.savetxt(\"book_gensim_test_v1.csv\", yX_test, delimiter=\",\", fmt='%0.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='book_gensim_train_v1.csv', key_prefix='%s/data' % prefix_gensim)\n",
    "input_validation = sagemaker_session.upload_data(path='book_gensim_test_v1.csv', key_prefix='%s/data' % prefix_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_data=input_train,content_type=\"csv\")\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_data=input_validation,content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the script already some some defaults specified, these will overwrite\n",
    "hyperparams = {\n",
    "        \"n_estimators\": \"1000\", \n",
    "        \"n_jobs\":\"4\",\n",
    "        \"learning_rate\": \"0.01\", \n",
    "        \"objective\":'multi:softmax', \n",
    "        \"subsample\": \"1\", \n",
    "        \"reg_lambda\": \"0.1\",\n",
    "        \"colsample_bytree\": \"1\",\n",
    "        \"gamma\": \"1\",\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated XGBoost to XGBClassifier https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html#train-a-model-with-open-source-xgboost\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparams,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    framework_version=\"1.2-1\",\n",
    "    eval_metric=\"merror\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator.fit({'train': train_data, 'validation': validation_data })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_gensim = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "csv_serializer = CSVSerializer()\n",
    "csv_deserializer = CSVDeserializer()\n",
    "\n",
    "xgb_predictor_gensim.serializer = csv_serializer\n",
    "xgb_predictor_gensim.deserializer = csv_deserializer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_gensim.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_xgb_weighted = xgb_predictor_gensim.predict(X_test) \n",
    "score = f1_score(y_test,predictions_test_xgb_weighted,labels=np.unique(y),average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-default",
   "metadata": {},
   "source": [
    "### In the next steps, we will use the built-in XGBoost which doesn't allow you to set the weights for the classes and see how the results differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_gensim_xgb, val_gensim_xgb = train_test_split(df_gensim_xgboost_imbalanced, test_size=0.25, random_state=24)\n",
    "\n",
    "train_gensim_xgb_file_name = 'books_xgb_train.csv'\n",
    "valid_gensim_xgb_file_name = 'books_xgb_valid.csv'\n",
    "train_gensim_xgb.to_csv(train_gensim_xgb_file_name, index=False, header=False)\n",
    "val_gensim_xgb.to_csv(valid_gensim_xgb_file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to an S3 bucket\n",
    "input_gensim_xgb_train = sagemaker_session.upload_data(path=train_gensim_xgb_file_name, key_prefix='%s/data' % prefix_gensim)\n",
    "input_gensim_xgb_test = sagemaker_session.upload_data(path=valid_gensim_xgb_file_name, key_prefix='%s/data' % prefix_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_gensim_xgb_data = TrainingInput(s3_data=input_gensim_xgb_train,content_type=\"csv\")\n",
    "test_gensim_xgb_data = TrainingInput(s3_data=input_gensim_xgb_test,content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-aircraft",
   "metadata": {},
   "source": [
    "If we use the XGBClassifer, then we are going to need to divide our training data into 3 files, X =features, y=Labels, and W=weights - all the same length. \n",
    "\n",
    "We are going to need to cerate a map to class to add the weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "container_uri = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, version='1.0-1')\n",
    "\n",
    "# Create the estimator\n",
    "xgb_bi = sagemaker.estimator.Estimator(container_uri,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix_gensim),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "# Set the hyperparameters\n",
    "xgb_bi.set_hyperparameters(eta=0.1,\n",
    "                        max_depth=10,\n",
    "                        gamma=4,\n",
    "                        num_class=len(np.unique(df_gensim_xgboost_imbalanced['cat_x2_code'])),\n",
    "                        alpha=10,\n",
    "                        min_child_weight=6,\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bi.fit({'train': train_gensim_xgb_data, 'validation': test_gensim_xgb_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-honor",
   "metadata": {},
   "source": [
    "# We trained our model and now want to test out the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_bi.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.metrics import f1_score\n",
    "csv_serializer = CSVSerializer()\n",
    "\n",
    "endpoint_name2 = xgb_predictor.endpoint_name\n",
    "model_name2 = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name2\n",
    ")['ProductionVariants'][0]['ModelName']\n",
    "!echo $model_name2 > model_name2.txt\n",
    "!echo $endpoint_name2 > endpoint_name2.txt\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_s = np.array(data_subset_2_cat_x2_agg_weights[\"cat_x2_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = xgb_predictor.predict(X_test_array[1])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-sleeping",
   "metadata": {},
   "source": [
    "XGBoost was actually able to predict the same class without the sampling, but we should check some metrics to compare performance between the two such as F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [ float(xgb_predictor.predict(x).decode('utf-8')) for x in X_test_array] \n",
    "score = f1_score(y_test,predictions_test,labels=label_s,average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = pd.DataFrame(predictions_test)\n",
    "p_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-surface",
   "metadata": {},
   "source": [
    "The built-in XGBoost where you are not able to set any sample_weight parameter did not handle the training very well, and actually just predicts that every result is the majority class which is class 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "csv_serializer = CSVSerializer()\n",
    "\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name2,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_serializer.serialize(X_test_array[0])\n",
    ")\n",
    "prediction = resp['Body'].read().decode('utf-8')\n",
    "#print('Predicted class: %.1f for [%s]' % (prediction, X_test_array[0]) )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-blues",
   "metadata": {},
   "source": [
    "All done, you can delete your endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-leonard",
   "metadata": {},
   "source": [
    "# Next we will test out the FastText native supervised Text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-auckland",
   "metadata": {},
   "source": [
    "In this step, we want to see if the native FastText algorithm is able to do the same but with less hard work.\n",
    "With native FastText, you do not need to tokenize your sentences, and you also do not need to pick vector size as a parameter for the mdoel training. \n",
    "This algorithm will do the work for you behind the scenes. \n",
    "What we do need to do though, is get the data in to the required format which means adding a string of \"__label__\" before the label and then we will concatenate that with the description and title into one field and then present that to the algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext['full'] = df_fasttext['fastText_label'] + ' ' + df_fasttext['description_str'] + ' ' + df_fasttext['title'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fasttext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-indonesia",
   "metadata": {},
   "source": [
    "Taken the same index as our test example above to see if the fasttext algo can make the same prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ft_x=df_fasttext['full'].iloc[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fasttext==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_dataset = df_fasttext['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_fasttext_native, val_fasttext_native = train_test_split(fasttext_dataset, test_size=0.25, random_state=24)\n",
    "\n",
    "train_file_name = 'train_books_fasttext_native.csv'\n",
    "valid_file_name = 'valid_books_fasttext_native.csv'\n",
    "train_fasttext_native.to_csv(train_file_name, index=False, header=False)\n",
    "val_fasttext_native.to_csv(valid_file_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native = fasttext.train_supervised(input=train_file_name, lr=0.001, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwordGram = fasttext.train_supervised(input=train_file_name, lr=0.001, epoch=50, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-rehabilitation",
   "metadata": {},
   "source": [
    "### We will run a simple test with the validation data, we are returned the precision and recall, adn we can play with the hyper parameters to tune this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native.test(valid_file_name, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwordGram.test(valid_file_name, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2*((0.17925272685077745*0.8962636342538872)/ (0.17925272685077745+0.8962636342538872))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-bible",
   "metadata": {},
   "source": [
    "## Test the prediction versus what we got with the xgb classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-growing",
   "metadata": {},
   "source": [
    "The model was able to return the same result of 17. We get a probability of 95% for this label - which is good!  and for the wordGram model that we trained, the probability score is even higher at 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_native.predict(val_ft_x, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwordGram.predict(val_ft_x, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-carpet",
   "metadata": {},
   "source": [
    "We can host our model on SageMaker. Blazing Text built-in algorithm is compatible with Fasttext's models, so we can upload the fastText model to S3 and then point a SageMaker endpoint configuration to this model, and then deploy our endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"books_fasttext_native.bin\"\n",
    "model_native.save_model(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf books_fasttext_native.tar.gz books_fasttext_native.bin\n",
    "model_location = sagemaker_session.upload_data(\"books_fasttext_native.tar.gz\", bucket=bucket, key_prefix='fasttext/model')\n",
    "!rm books_fasttext_native.tar.gz books_fasttext_native.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"blazingtext\",region,  \"1\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-comment",
   "metadata": {},
   "source": [
    "# deploy endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use blazing text container and the fasttext model\n",
    "model_fastText_book = sagemaker.Model(\n",
    "    model_data=model_location, \n",
    "    image_uri=container, \n",
    "    role=role, \n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "model_fastText_book.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sklearn.metrics import accuracy_score\n",
    "json_serializer = JSONSerializer()\n",
    "\n",
    "endpoint_name3 = model_fastText_book.endpoint_name\n",
    "model_name3 = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name3\n",
    ")['ProductionVariants'][0]['ModelName']\n",
    "!echo $model_name3 > model_name3.txt\n",
    "!echo $endpoint_name3 > endpoint_name3.txt\n",
    "model_fastText_book.serializer = json_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-sewing",
   "metadata": {},
   "source": [
    "We will use the same validation sample as above to make sure that we are returned label 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker-runtime')\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name3,\n",
    "    ContentType='application/json',\n",
    "    Body=json_serializer.serialize(val_ft_x)\n",
    ")\n",
    "prediction = resp['Body'].read().decode('utf-8')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-determination",
   "metadata": {},
   "source": [
    "# Clean up, delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_gensim.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-insert",
   "metadata": {},
   "source": [
    "If following these steps, make sure to check the console for any extra endpoints that you do not need and delete them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-latest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
